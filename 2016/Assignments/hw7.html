<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Homework 7</title>
  <meta name="description" content="Neural Signal Procession and Machine Learning">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/Assignments/hw7.html">
  <link rel="alternate" type="application/rss+xml" title="ELEC548 - Neural Signal Processing and Machine Learning" href="http://localhost:4000/feed.xml">
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">

    <a class="site-title" href="/">ELEC548 - Neural Signal Processing and Machine Learning</a>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <h2 id="homework-7-100-pts">Homework 7 (100 pts)</h2>

<p><em>This problem set is due Tuesday (11/29/2016) at 11:59pm. Please turn in your
work by uploading to Canvas. f you have questions, please post them on the
course forum, rather than emailing the course staff. This will allow other
students with the same question to see the response and any ensuing discussion.
The goal of this problem set is to review clustering through the process of
spike sorting action potentials data. You should submit both commented code and
a write-up for this homework assignment.</em></p>

<h3 id="description-of-data-sets">Description of data sets</h3>
<p>For Problem 1 of this assignment, you will again use the reach data from HW 5 (in file
<code class="highlighter-rouge">ReachData.npz</code> on Canvas).  For Problem 2, you will
The dataset for this problem is linked in the numpy file
<a href="SpikeData05.npz">SpikeData05.npz</a>, which contains numpy arrays
<code class="highlighter-rouge">SpikeWaveforms</code> and <del><code class="highlighter-rouge">RawData</code></del>. (The raw data was too large to upload. It
is available on request.) You can load it by calling</p>

<ol>
  <li>
    <p><em>Visualization of high-dimensional neural activity using PCA (30 pts)</em>
  For this problem, you will again use the reach data from HW 5 (in file
  <code class="highlighter-rouge">ReachData.npz</code> on Canvas). Begin by extracting the “plan” data for all trials
  and all targets. Use a constant length window 200 ms long starting 50 ms after
  the timeTouchHeld. You should end up with a matrix of <script type="math/tex">N \times K</script>, where
  <script type="math/tex">N</script> is the number of neurons and <script type="math/tex">K</script> is the total number of trials.</p>

    <p><strong>a.</strong> Calculate the principal components of the neural activity (there is a
 PCA decomposition within sklearn, but you can also just use the
 <code class="highlighter-rouge">numpy.linalg.eig</code> function). Plot the square-rooted eigenvalue spectrum.
 If you had to identify an elbow in the eigenvalue spectrum, how many
 dominant eigenvalues would there be? What percentage of the overall
 variance is captured by the top 3 principal components?</p>

    <p><strong>b.</strong> For the purposes of visualization, we’ll consider the PC space defined by
 the top M = 3 eigenvectors. Project the data into the three-dimensional PC
 space. Plot the projected points in a 3D plot
 (<a href="http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html">example here</a>) and
 color each dot appropriately according to reaching target. Rotate the axes
 (<a href="http://matplotlib.org/examples/mplot3d/rotate_axes3d_demo.html">example here</a>)
 to show a view in which the clusters are well-separated.</p>

    <p><strong>c.</strong> Define a matrix <script type="math/tex">U_M \in \mathbb{R}^{D \times M}</script> containing the
 top three eigenvectors (i.e., PC directions), where <script type="math/tex">U_M (d, m)</script> indicates
 the contribution of the <script type="math/tex">d</script>th neuron to the <script type="math/tex">m</script>th principal component.
 Show the values in <script type="math/tex">U_M</script>, e.g., by calling <code class="highlighter-rouge">imshow(UM )’. (Note: Also
 call </code>colorbar’ to show the scale.) Are there are any obvious groupings
 among the neurons in each column of <script type="math/tex">U_M</script>?</p>
  </li>
  <li>
    <p><em>Reducing the dimensionality of spike waveforms (40 pts)</em>
  For this problem, you will again use the spiking data from HW 6.</p>

    <p><strong>a.</strong> Begin with the data from the numpy file
 <a href="SpikeData05.npz">SpikeData05.npz</a>. Use the first 5000 samples. Calculate
 the principal components of the spike waveforms on the first tetrode channel.
 Plot the square-rooted eigenvalue spectrum. If you had to identify an elbow
 in the eigenvalue spectrum, how many dominant eigenvalues would there be?
 What percentage of the overall variance is captured by the top 3 principal
 components?</p>

    <p><strong>b.</strong> Plot the mean and the first 2 principal components. What do they
 look like?</p>

    <p><strong>c.</strong> Now repeat the process for the first 5000 samples in
 <a href="SpikeData12.npz">SpikeData12.npz</a>, again just using the first tetrode
 channel. Are there any major differences?</p>

    <p><strong>d.</strong> Now, consider all the channels in SpikeData12 as a single, 160x1
 vector. Calculate the principal components of this larger vector. Plot
 the square-rooted eigenvalues - how many dominant eigenvalues do you
 think there are?</p>

    <p><strong>e.</strong> Plot the mean and the first 2 principal components of the tetrode
 principal components. How do the values for the first channel differ
 from the values in the single channel analysis from (c)? What is the
 difference in the fraction of the variance in the first tetrode channel
 captured by the 4 channel PCA vs. the single channel PCA? What about
 the first three principal components? <em>Hint: Only consider
 the first tetrode channel. Calculate the total variance in the samples,
 then project into the space of the first two principal components (either
 single channel or full data). Back-project these into waveform space
 by multiplying by the principal component vectors and adding the results.
 Add the mean to get back to the original. Now calculate the variance (only
 for the first tetrode channel!) of these effectively lower dimensional
 waveforms, and take the ratio to the baseline.)</em></p>
  </li>
  <li>
    <p><em>Comparing PCA, P-PCA, and FA (30 pts)</em>
 To answer these questions, please begin by downloading and running the
 Jupyter notebook for the problem
 <a href="https://elec548.github.io/Assignments/Homework7Problem3.ipynb">Homework7Problem3.ipynb</a>.
 For PCA and P-PCA, the notebook makes a plot showing:</p>
    <ul>
      <li>Each data point <script type="math/tex">x_n</script> as a black dot in its native 2-D space</li>
      <li>The mean, $\mu$  as a green point</li>
      <li>The PC space found by either PCA or P-PCA (which is one dimensional)
   plotted as a line segment</li>
      <li>The projection of each data point into the PC space as red points</li>
      <li>A red line connecting the original and lower dimensional data</li>
    </ul>

    <p><strong>a.</strong> The lower dimensional space found by PCA and P-PCA are the same.
 So why are the lines in P-PCA not orthogonal as they are in the PCA
 case?</p>

    <p><strong>b.</strong> Using the code for P-PCA as a guide, implement the EM algorithm
 for Factor Analysis and make a plot showing the same information. Note that
 the lower dimensional space found by FA will pass through the mean but
 will not be the same as in PCA/P-PCA. Why is it different?</p>
  </li>
</ol>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">
    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
          &copy; 2016 <a href="mailto:caleb.kemere@rice.edu">Caleb Kemere</a>
      </div>
    </div>

  </div>

</footer>


  <script type="text/javascript"
     src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
  </body>

</html>
